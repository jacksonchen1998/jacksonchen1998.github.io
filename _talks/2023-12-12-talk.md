---
title: "On the Effectiveness of Offline RL for Dialogue Response Generation"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-12-12-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-10-24
location: "Hsinchu city, Taiwan"
---

For language models, many methods using teacher forcing (TF) to train.

But with offline RL, which shows a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training budgets.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/fDcynWW9bmiLex?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
- [Offline Reinforcement Learning with Implicit Q-Learning](https://arxiv.org/abs/2110.06169)
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [A Learning Algorithm for Continually Running Fully Recurrent Neural Networks](https://doi.org/10.1162/neco.1989.1.2.270)