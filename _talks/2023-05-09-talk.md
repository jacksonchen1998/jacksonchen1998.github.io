---
title: "Evaluating Parameter Efficient Learning for Generation"
collection: talks
type: "NLG Group Meeting"
permalink: /talks/2023-05-09-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-05-09
location: "Hsinchu city, Taiwan"
---

In this paper, they present a comprehensive evaluation of parameter efficient learning methods (PERMs) for generation tasks in natural
language processing.

They compare PERMs to finetuning from three new perspectives, including
1. The impact of sample and model size
2. Generalization to unseen domains and datasets
3. Faithfulness of generations

Their results show that PERMs can outperform finetuning in certain scenarios, particularly when training with fewer samples and using larger pre-trained language models.

This study provides valuable insights into the effectiveness of PERMs for adapting pre-trained language models to downstream tasks.

Powerpoint for this talk
=====
[Powerpoint for this talk](https://www.slideshare.net/jacksonChen22/evaluating-parameter-efficient-learning-for-generationpdf)

Reference Paper
=====
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [Prefix-tuning: Optimizing continuous prompts for generation](https://arxiv.org/abs/2101.00190)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586v1.pdf)
- [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
