---
title: "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-07-25-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-06-27
location: "Hsinchu city, Taiwan"
---

This paper proposes **LLaMA-Adapter**, a lightweight adaption method to efficiently finetune LLaMA into an instruction-following model.

Specifically, they adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers.

Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge.

LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters.

Also, it can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/i8qUkL6H1BSSk?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010)
- [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)
