---
title: "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"
collection: talks
type: "Lab Seminar"
permalink: /talks/2023-07-27-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-07-27
location: "Hsinchu city, Taiwan"
---

This paper proposes a parameter-efficient fine-tuning method called $\texttt{AdaMix}$, a general parameter-efficient fine-tuning (PEFT) techniques that tunes a mixture of adaptation modules.

By only tuning $0.1 âˆ’ 0.2\%$ of PLM parameters, they show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/9WIe5Tpl1B90Wx?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247)
