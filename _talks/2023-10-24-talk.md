---
title: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-10-24-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-10-24
location: "Hsinchu city, Taiwan"
---

Nowadays, Generative Pre-trained Transformer models has not only breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs.

While there is emerging work on relieving this pressure via **model compression**, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models.

In this paper, they propose **GPTQ, a new one-shot weight quantization method based on approximate second-order information.**

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/2h4sgUrSAgjbjw?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://arxiv.org/abs/2208.11580)
- [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://arxiv.org/abs/2206.01861)
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)