---
title: "HyperPrompt:Prompt-based Task-Conditioning of Transformers"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-03-07-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-03-07
location: "Hsinchu city, Taiwan"
---

This paper proposes HyperPrompt, a novel architecture for prompt-based task-conditioning of self-attention in Transformers.

HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories for the queries to attend to, at the same time enabling flexible information sharing among tasks.

HyperPrompt is competitive against strong multi-task learning baselines with as few as 0.14% of additional task-conditioning parameters, achieving great parameter and computational efficiency.

Powerpoint for this talk
=====
[Powerpoint for this talk](https://www.slideshare.net/jacksonChen22/hyperpromptpromptbased-taskconditioning-of-transformerspdf)

Reference Paper
=====
- [HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning](https://arxiv.org/pdf/2201.04182.pdf)
- [Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks](https://arxiv.org/pdf/2102.02017.pdf)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
