---
title: "Training language models to follow instructions with human feedback"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-01-10-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-01-10
location: "Hsinchu city, Taiwan"
---
This paper show a method to **align language models with user intent** on a wide range of tasks by fine-tuning with human feedback.

With using labeler-written prompts and prompts submitted through the **OpenAI API** and **a dataset of labeler demonstrations of the desired model behavior.** Such that they can fine-tune GPT-3 using supervised learning.

And then collect **a dataset of rankings of model outputs**, which they use to further fine-tune this supervised model using **reinforcement learning from human feedback**, the model called *InstructGPT*.

Recommend tutorials for this paper
======
* [[Mandarin] About GPT, GPT-2, GPT-3](https://youtu.be/t70Bl3w7bxY)
* [[Mandarin] About InstructGPT](https://youtu.be/zfIGAwD1jOQ)

Recommend website for this paper
======
* [ChatGPT is multilingual but monocultural, and itâ€™s learning your values](https://jilltxt.net/right-now-chatgpt-is-multilingual-but-monocultural-but-its-learning-your-values/)
* [GPT-3.5 + ChatGPT: An illustrated overview](https://lifearchitect.ai/chatgpt/)

Powerpoint for this talk
======
[Powerpoint for this talk](https://www.slideshare.net/jacksonChen22/training-language-models-to-follow-instructionswith-human-feedbackpdf)