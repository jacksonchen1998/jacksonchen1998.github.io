---
title: "Quark: Controllable Text Generation with Reinforced [Un]learning"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2024-04-09-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2024-04-09
location: "Hsinchu city, Taiwan"
---

Large language models may generate content that is misaligned with the userâ€™s expectations. For example, generating toxic words, repeated content, and undesired responses for users.

This paper addresses this challenge with an algorithm for optimizing a reward function that quantifies an (un)wanted property.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/FBH0ZO9stl92c?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)