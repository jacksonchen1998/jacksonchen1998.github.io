---
title: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2023-05-30-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2023-05-30
location: "Hsinchu city, Taiwan"
---

This study proposes a fine-tuning recipe for retrieval-augmented generation (RAG) models that combine pre-trained parametric and non-parametric memory for language generation. 

By using a pre-trained neural retriever to access a dense vector index of Wikipedia, RAG models outperform parametric seq2seq models and task-specific architectures on knowledge-intensive NLP tasks, including open-domain QA. 

RAG models also demonstrate the ability to generate more specific, diverse, and factual language compared to parametric-only seq2seq models.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/L88ekc1SuSuK7U?hostedIn=slideshare&page=upload" width="100%" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

Reference Paper
=====
- [Dense Passage Retrieval for Open-Domain Question Answering](https://aclanthology.org/2020.emnlp-main.550)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://aclanthology.org/2020.acl-main.703)
- [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983)