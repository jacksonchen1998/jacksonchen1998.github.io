---
title: "Graph Neural Prompting with Large Language Models"
collection: talks
type: "RL Group Meeting"
permalink: /talks/2024-06-04-talk
venue: "Hsinchu city, National Yang Ming Chiao Tung University"
date: 2024-06-04
location: "Hsinchu city, Taiwan"
---

To reduce the limitation of the large language model, the existing work enhances pre-trained LLMs using grounded knowledge.

For example, retrieval-augmented generation, remains an open question, knowledge graphs (KG).

In this paper, they propose **graph neural prompting (GNP)**, a plug-and-play method to help pre-trained LLMs gain beneficial knowledge from KGs.

Powerpoint for this talk
=====
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/M5vwZpPEfMpwnO?startSlide=1" width="100%" height="486" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px;max-width: 100%;" allowfullscreen></iframe>

Reference Paper
=====
- [Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering](https://aclanthology.org/2023.nlrse-1.7/)
- [Deep Bidirectional Language-Knowledge Graph Pretraining](https://papers.nips.cc/paper_files/paper/2022/hash/f224f056694bcfe465c5d84579785761-Abstract-Conference.html)
